# auto-pack-ui


Программная реализация

Разработанное программное решение представляет собой модульную систему, ориентированную на экспериментальное исследование различных подходов к задаче двумерной упаковки объектов. Архитектура проекта организована в виде трех основных компонентов, каждый из которых отвечает за определенный аспект функциональности:
“packing_lib” – ядро проекта, включающее алгоритмы упаковки, обработку изображений и распознавание объектов. Модуль спроектирован с упором на расширяемость и повторное использование в других компонентах системы.
“app” –  приложение с графическим интерфейсом на базе Pygame, предоставляющее пользовательский экраны для настройки, запуска и визуализации упаковки.
“benchmark” – вспомогательный модуль для проведения экспериментов: включает метрики оценки качества, сценарии сравнения алгоритмов и средства визуального анализа результатов.
Каждый компонент разрабатывался независимо, с возможностью вызова функций из других модулей, что обеспечивает гибкость и масштабируемость архитектуры. Далее представлены подробности реализации каждого из модулей, включая ключевые классы, взаимодействия между частями системы, используемые внешние библиотеки и принципы организации кода.

Реализация модуля packing_lib

В библиотеке “packing_lib” реализация алгоритмов упаковки организована на основе чётко заданного интерфейса, обеспечивающего единообразие во взаимодействии и поддержку модульности. В качестве основы используется абстрактный контракт, который определяет обязательный набор входных и выходных типов, применимый ко всем стратегиям размещения объектов. Такой подход позволяет легко добавлять и тестировать новые алгоритмы, не внося изменений в остальной код.
Типы входных данных и результатов упаковки представлены в виде структур, описывающих объекты, контейнер и результат размещения. Это обеспечивает строгую типизацию, согласованность форматов данных и однозначность их интерпретации. Алгоритмы размещения инкапсулированы в отдельных классах, каждый из которых реализует общий интерфейс. За счет единых соглашений между компонентами система допускает свободную замену или комбинирование алгоритмов без необходимости вносить изменения в остальную логику.
Класс “PhysicsEngine” инкапсулирует все аспекты физического моделирования: настройку пространства, создание границ контейнера, генерацию тел и их размещение. Управление параметрами осуществляется через конфигурационную структуру “PhysicsConfig”, где задаются гравитация, коэффициенты трения, масса тел, чувствительность к движениям и другие параметры симуляции. Состояние тел отслеживается при помощи трекеров (“BodyTracker”), фиксирующих движение и угол поворота каждого объекта. Это позволяет определять момент стабилизации системы и переходить к следующим фазам алгоритма (разворот объектов, встряхиванию контейнера и заполнению пустот).
В дополнение к размещению, движок используется и для оценки устойчивости упаковки: анализируется смещение объектов при встряхивании, что служит базой для одной из разработанных метрик.
Для преобразования изображения с камеры в структуру данных, пригодную для упаковки, в системе используются два детектора: один для извлечения объектов, подлежащих размещению, и другой для определения масштаба и ориентации объектов в мировых координатах.
Класс “YoloBoxDetector” отвечает за выделение объектов на изображении. В качестве модели используется модель сегментации FastSAM, интегрированный через фреймворк Ultralytics YOLO [38], предоставляющий унифицированную инфраструктуру для запуска моделей детекции, классификации и сегментации. Несмотря на название, в данном случае речь идёт не о классической модели YOLO (You Only Look Once) [39], а о программной оболочке, поддерживающей разнообразные архитектуры. FastSAM [40] представляет собой упрощенную и ускоренную реализацию идеи Segment Anything Model (SAM) [41], предложенной исследовательской группой Meta AI, и предназначен для выполнения семантической сегментации в условиях ограниченных вычислительных ресурсов. На выходе формируются маски объектов, из которых извлекаются минимальные повернутые ограничивающие прямоугольники.
Класс “ArucoBoxDetector” выполняет обнаружение маркеров, размещенных в сцене, и используется для масштабной и геометрической калибровки изображения. На основе известного размера маркера определяется масштабный коэффициент (миллиметры на пиксель), позволяющий перевести координаты из пиксельного пространства в метрическое. Также ArUco-детектор обеспечивает переход к мировой системе координат: центр одного из маркеров принимается за начало отсчета, что позволяет формировать унифицированные описания объектов независимо от положения камеры. 
Класс “SceneProcessor” объединяет данные от обоих детекторов, выполняет фильтрацию пересекающихся объектов, масштабирует координаты в миллиметры и формирует финальное представление сцены в виде списка структур “RectObject”. При этом осуществляется проверка корректности расположения маркеров и геометрической согласованности сцены. Для успешной привязки координат к реальному пространству требуется наличие как минимум трех базовых ArUco-маркеров, расположенных по углам рабочей области. Верхний левый маркер принимается за начало отсчета, и в случае отсутствия одного или нескольких базовых маркеров переход к метрической системе координат будет невозможен.
Реализация модуля app

Модуль “app” представляет собой интерактивное графическое приложение, реализованное на базе библиотеки Pygame [42] с использованием pygame_gui [43] для построения пользовательского интерфейса. Его основная задача заключается в предоставление визуальной оболочки для взаимодействия с алгоритмами упаковки, настройки параметров и отображения результатов работы.
В качестве основы приложения используется класс “App”, управляющий инициализацией среды, циклом отрисовки и обработкой пользовательских событий. Приложение построено по принципу разделения на экраны, каждый из которых отвечает за отдельную фазу взаимодействия (настройка, визуализация, симуляция и т.д.). Переходы между экранами контролируются через “ScreenManager”, который отслеживает, какой из экранов в данный момент активен, и перенаправляет ему события ввода (нажатия клавиш, движения мыши и т.д.), а также управляет отображением нужного интерфейса на экране. Все экраны наследуются от базового класса “ScreenBase”, в котором определен стандартный интерфейс для отрисовки, обработки событий и масштабирования интерфейса при изменении размеров окна.
Глобальный контекст приложения формируется через класс “AppContext”, который агрегирует все ключевые зависимости: окно отрисовки (“surface”), менеджер интерфейса (“ui_manager” из библиотеки pygame_gui), менеджер экранов (“screen_manager”), настройки (“AppConfig”) и контроллер камеры. Такая организация обеспечивает централизованный доступ к ресурсам и избавляет от необходимости явно передавать параметры между экранами. Конфигурационные параметры, такие как URL видеопотока, размеры контейнера и параметры калибровки камеры, инкапсулированы в модуле “AppConfig”, что позволяет управлять настройками приложения из единой точки и гибко адаптировать поведение системы под разные сценарии использования.
Захват и распознавание видео реализованы в классе “CameraController”, который управляет подключением к камере, получением кадров и распознаванием объектов в режиме реального времени. Система поддерживает потоковое подключение к любому источнику, совместимому с OpenCV (например, USB-камеры или IP-камеры). В рамках текущей реализации в качестве камеры использовался мобильный телефон с установленным приложением DroidCam [44], это позволило обеспечить беспроводную передачу видеопотока с минимальной настройкой.
Обработка видео реализована с использованием многопоточности: один поток отвечает за непрерывное считывание кадров с камеры, второй за распознавание объектов. Эти процессы работают независимо друг от друга. Поток захвата постоянно обновляет актуальный кадр, а поток детекции выполняет анализ только тогда, когда завершил предыдущий шаг. Таким образом, при каждом новом запуске распознавания в обработку поступает самый новый доступный кадр, а промежуточные кадры, пришедшие во время работы алгоритма, пропускаются. Это позволяет системе эффективно использовать ресурсы и сохранять отклик интерфейса даже при использовании вычислительно затратной модели сегментации.
Для синхронизации потоков применяется механизм блокировок, исключающий конфликты при доступе к общим данным. Обнаруженные объекты, маркеры и координаты, пересчитанные в метрические единицы, доступны через соответствующие интерфейсы.
Логика отображения результатов, управления сценой и взаимодействия с пользователем организована через набор отдельных экранов. Каждый из них сфокусирован на своей задаче: от конфигурации параметров до запуска симуляции и анализа результатов упаковки. В дальнейшем эти экраны будут рассмотрены подробнее с сопровождением иллюстраций.
Экран конфигурации (“ConfigScreen”) используется на первом этапе работы и позволяет задать ключевые параметры упаковки. Визуально представлен формой с полями для пользовательского ввода из библиотеки pygame_gui. Все изменения применяются через “AppConfig”. Этот экран обеспечивает начальную инициализацию системы и подготавливает данные для дальнейших шагов.
Главный экран (“MainScreen”) представляет собой основной интерфейс взаимодействия пользователя с системой в режиме реального времени. Его визуальная область разделена на три функциональных части. Слева расположено рабочее пространство, в котором отображается изображение с камеры. Здесь же выводится текстовое поле, информирующее о текущем состоянии видеопотока и контроллера камеры. После запуска распознавания в этой области отображается разметка сцены: контуры распознанных объектов (ограничивающие прямоугольники), центры захвата и обнаруженные ArUco-маркеры. Центральную часть занимает визуализация контейнера, в которую помещаются объекты после упаковки. Здесь пользователь может наблюдать результат работы алгоритма: расположение объектов, ориентации и заполнение доступного пространства. Эта область обновляется после фиксации сцены и запуска упаковки. Справа находится панель управления, включающая набор кнопок и элементов интерфейса. С её помощью можно управлять состоянием камеры, запускать или останавливать обработку кадров, выбирать алгоритм упаковки (эвристический или физический), фиксировать текущую сцену, а также перейти к настройке параметров или сохранению результатов. На рисунке 7 представлен скриншот главного экрана пользовательского интерфейса с областями визуализации и управления, на левой стороне экрана находится рабочее пространство с размеченными объектами, а в центральной части визуализация упакованных объектов, размещенных методом NFDH.


Рисунок 7 – Главный экран

Экран “PhysScreen” предназначен для отображения процесса физической упаковки в реальном времени. Пользователь попадает на него после выбора соответствующего метода. Здесь в центре экрана визуализируется работа физического движка: объекты падают и взаимодействуют согласно законам механики, постепенно заполняя контейнер.
На рисунке 8 представлена схема работы разработанной системы упаковки. Рабочий процесс начинается с подключения к IP-камере и запуска цикла получения и обработки видеопотока. Для обеспечения интерактивности и устойчивости интерфейса процессы захвата изображения и распознавания объектов реализованы в отдельных потоках, работающих параллельно.

Рисунок 8 – Схематическое представление алгоритма

После получения кадра система выполняет распознавание объектов и ArUco-маркеров. Далее ожидается действие пользователя по фиксации кадра. Этот шаг служит сигналом к переходу от непрерывного режима детекции к этапу масштабирования распознанных объектов.
Масштабирование осуществляется с использованием информации от ArUco-маркера: известный физический размер маркера позволяет преобразовать координаты из пиксельного пространства в метрическую систему. После преобразования пользователь может инициировать упаковку - выбрав один из доступных алгоритмов.

Рисунок 9 – Результаты распознавания объектов и ArUco-маркеров на захваченном кадре сцены

На рисунке 9 показан пример захваченного кадра, в котором визуализированы результаты работы системы: зелеными рамками обозначены распознанные объекты, а синими – ArUco-маркеры, используемые для калибровки сцены. Также отображены идентификаторы объектов, а также их координаты и размеры в метрической системе. Голубая стрелка, ведущая от левого верхнего маркера к правому, указывает направление оси x мировой системы координат.
Результатом работы упаковщика является файл с координатами размещения объектов. Структура выходных данных была заранее определена в третьей главе данной работы. Данные формируется в формате JSON файла и содержат необходимые параметры для каждого объекта: координаты захвата, ориентацию и целевую позицию укладки. Подобный формат пригоден для передачи в системы управления промышленными манипуляторами.
Также предусмотрен режим генерации случайных данных, позволяющий запускать алгоритмы без подключения к камере для целей тестирования и отладки.
Для наглядного представления возможностей системы на рисунке 10 приведена диаграмма вариантов использования. Она иллюстрирует основные действия, доступные пользователю: ввод параметров, управление камерой, выбор метода упаковки и сохранение результатов.


Рисунок 10 –  Диаграмма use case

Модуль benchmark и эксперименты

Для систематической оценки эффективности разработанных алгоритмов упаковки был реализован отдельный модуль, обеспечивающий массовый запуск сценариев, сбор метрик и визуализацию результатов. Это позволяет количественно сравнивать различные подходы по ключевым показателям, а также анализировать стабильность решений при повторных запусках. Модуль “benchmark” включает следующие ключевые компоненты:
Класс “Benchmark” – основной контроллер, запускающий серию упаковок по заданному сценарию и алгоритму. Он отвечает за генерацию или загрузку входных данных, выполнение упаковки, а также вызов функций для расчета метрик.
“PackingData” – структура, инкапсулирующая исходные параметры упаковки и полученные результаты. Обеспечивает единый интерфейс для доступа к данным объектов, контейнера и алгоритма.
“Drawer” – вспомогательный модуль, реализующий отрисовку результатов упаковки с использованием `matplotlib`. Предоставляет как визуализацию отдельных упаковок, так и групповые диаграммы по папке JSON-файлов.
Все метрики реализованы как независимые классы с единым интерфейсом: каждой передается структура “PackingData”, содержащая как входные параметры (контейнер, объекты), так и результаты упаковки. После вызова метода “calculate()” возвращается численное значение, соответствующее оценке текущей укладки. В частности:
“PlacementDensityCalculator” вычисляет долю занятой площади контейнера, используя параметры размещенных объектов.
“CenterOfMassShiftCalculator” определяет смещение центра масс от геометрического центра контейнера. В реализации используется нормировка по диагонали, чтобы значения были сопоставимыми для контейнеров разных размеров.
“ShakeStabilityCalculator” производит запуск физической симуляции на основе библиотеки Pymunk: к уже размещенным объектам применяются гравитационные и псевдослучайные силы, после чего рассчитывается среднее смещение объектов. Реализация включает внутреннюю обработку столкновений и временной шаг симуляции.
Каждая метрика возвращает числовое значение, которое либо сразу отображается в отчетах, либо используется для агрегирования статистики по серии экспериментов. Помимо самих значений, предусмотрена возможность расчёта среднего и стандартного отклонения по результатам нескольких запусков, что особенно важно при тестировании недетерминированных алгоритмов (например, физической симуляции). Эти операции реализуются стандартными средствами Python (“statistics.mean”, “statistics.stdev”).
В целом, реализация метрик направлена на модульность, расширяемость и автоматическое включение в процесс тестирования, обеспечивая достоверную и повторяемую оценку качества работы упаковщиков.
Для количественной оценки эффективности разработанных алгоритмов была проведена серия экспериментов на фиксированном наборе входных данных. Каждый алгоритм запускался по 10 раз на одних и тех же сценах. Это позволило не только определить средние значения метрик, но и оценить стабильность решений с помощью стандартного отклонения.
Обработка результатов выполнялась автоматически с помощью скрипта “PackTests.py”, который считывал данные из подготовленных директорий:
“out/bench/dataset_test/greedy/” – результаты алгоритма NFDH;
“out/bench/dataset_test/exact/” – результаты переборного алгоритма;
“out/bench/dataset_test/phys/” – результаты физического алгоритма.
Для каждого набора запусков вычислялись агрегированные значения всех ключевых метрик: плотности размещения, смещения центра масс и устойчивости к встряхиванию. Полученные данные визуализировались с помощью модулей из “Drawer.py”, результаты в таблице 1.
